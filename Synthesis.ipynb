{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZCYrt3lND825"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define the Text-to-Image GAN model\n",
        "class TextToImageGenerator(nn.Module):\n",
        "    def __init__(self, z_dim=100, text_embedding_dim=768, img_channels=3):\n",
        "        super(TextToImageGenerator, self).__init__()\n",
        "        self.fc1 = nn.Linear(z_dim + text_embedding_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 512)\n",
        "        self.fc3 = nn.Linear(512, 1024)\n",
        "        self.fc4 = nn.Linear(1024, img_channels * 64 * 64)  # Generate 64x64 image\n",
        "        self.relu = nn.ReLU()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, z, text_embedding):\n",
        "        # Concatenate random noise with text embeddings\n",
        "        x = torch.cat((z, text_embedding), dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.tanh(self.fc4(x))\n",
        "        return x.view(-1, 3, 64, 64)  # Reshape to image dimensions\n",
        ""
      ],
      "metadata": {
        "id": "xf1uQ75GECfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the Discriminator model\n",
        "class TextToImageDiscriminator(nn.Module):\n",
        "    def __init__(self, text_embedding_dim=768):\n",
        "        super(TextToImageDiscriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 64 * 64 + text_embedding_dim, 1024)\n",
        "        self.fc2 = nn.Linear(1024, 512)\n",
        "        self.fc3 = nn.Linear(512, 256)\n",
        "        self.fc4 = nn.Linear(256, 1)  # Output single value: real or fake\n",
        "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, text_embedding):\n",
        "        x = x.view(-1, 3 * 64 * 64)  # Flatten the image\n",
        "        input = torch.cat((x, text_embedding), dim=1)  # Concatenate image and text\n",
        "        x = self.leaky_relu(self.fc1(input))\n",
        "        x = self.leaky_relu(self.fc2(x))\n",
        "        x = self.leaky_relu(self.fc3(x))\n",
        "        return self.sigmoid(self.fc4(x))\n",
        ""
      ],
      "metadata": {
        "id": "aKjINtQGEGGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load pre-trained BERT model for text embedding\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "text_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def text_to_embedding(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = text_model(**inputs)\n",
        "    return outputs.last_hidden_state.mean(dim=1)  # Use the mean of last layer hidden states\n",
        ""
      ],
      "metadata": {
        "id": "9K90ydkzEGJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Loss function and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "metadata": {
        "id": "ZVmxwA76EGMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Training loop for Text-to-Image GAN\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "generator = TextToImageGenerator().to(device)\n",
        "discriminator = TextToImageDiscriminator().to(device)\n",
        "\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (real_images, captions) in enumerate(train_loader):  # Assumes captions are paired with images\n",
        "        real_images = real_images.to(device)\n",
        "        text_embeddings = text_to_embedding(captions).to(device)\n",
        "\n",
        "        # Create labels for real and fake data\n",
        "        real_labels = torch.ones(real_images.size(0), 1).to(device)\n",
        "        fake_labels = torch.zeros(real_images.size(0), 1).to(device)\n",
        "\n",
        "        # Train the Discriminator\n",
        "        optimizer_d.zero_grad()\n",
        "\n",
        "        # Train on real images\n",
        "        real_outputs = discriminator(real_images, text_embeddings)\n",
        "        d_loss_real = criterion(real_outputs, real_labels)\n",
        "\n",
        "        # Train on fake images\n",
        "        z = torch.randn(real_images.size(0), 100).to(device)  # Random noise\n",
        "        fake_images = generator(z, text_embeddings)\n",
        "        fake_outputs = discriminator(fake_images.detach(), text_embeddings)\n",
        "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_d.step()\n",
        "\n",
        "        # Train the Generator\n",
        "        optimizer_g.zero_grad()\n",
        "        fake_outputs = discriminator(fake_images, text_embeddings)\n",
        "        g_loss = criterion(fake_outputs, real_labels)  # We want fake images to be classified as real\n",
        "        g_loss.backward()\n",
        "        optimizer_g.step()\n",
        "\n",
        "    # Print loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}')\n",
        "\n",
        "    # Generate and display sample images\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        with torch.no_grad():\n",
        "            z = torch.randn(8, 100).to(device)  # Random noise\n",
        "            captions = [\"a red apple on a table\"] * 8  # Example captions\n",
        "            text_embeddings = text_to_embedding(captions).to(device)\n",
        "            fake_images = generator(z, text_embeddings).cpu()\n",
        "            grid_img = torchvision.utils.make_grid(fake_images, nrow=4, normalize=True)\n",
        "            plt.imshow(grid_img.permute(1, 2, 0))\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "vNHvDwPhEGPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}